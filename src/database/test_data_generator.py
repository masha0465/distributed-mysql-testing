"""
ë„¤ì´ë²„ MySQL í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸°
ì‹¤ì œ ë„¤ì´ë²„ ì„œë¹„ìŠ¤ì™€ ìœ ì‚¬í•œ ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
"""
import asyncio
import random
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass

import mysql.connector
import aiomysql
from faker import Faker

@dataclass
class GenerationStats:
    """ë°ì´í„° ìƒì„± í†µê³„"""
    table_name: str
    target_count: int
    generated_count: int
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    generation_rate: float

class NaverStyleDataGenerator:
    """ë„¤ì´ë²„ ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸°"""
    
    def __init__(self, db_config: Dict):
        self.db_config = db_config
        self.fake_kr = Faker(['ko_KR'])
        self.fake_en = Faker(['en_US'])
        
        self.naver_domains = ['naver.com', 'hanmail.net', 'daum.net', 'gmail.com']
        self.blog_categories = [
            'ì¼ìƒ', 'ì—¬í–‰', 'ë§›ì§‘', 'íŒ¨ì…˜', 'ë·°í‹°', 'IT', 'ê²Œì„', 'ì˜í™”', 'ìŒì•…', 'ìŠ¤í¬ì¸ ',
            'ìš”ë¦¬', 'ìœ¡ì•„', 'íœí«', 'ìë™ì°¨', 'ë¶€ë™ì‚°', 'ì£¼ì‹', 'ì·¨ì—…', 'í•™ìŠµ', 'ê±´ê°•', 'ì—°ì• '
        ]
        self.search_keywords = [
            'ë‚ ì”¨', 'ë‰´ìŠ¤', 'ì½”ë¡œë‚˜', 'ì£¼ì‹', 'ë§›ì§‘', 'ì—¬í–‰', 'ì‡¼í•‘', 'ê²Œì„', 'ì˜í™”', 'ë“œë¼ë§ˆ',
            'ìŒì•…', 'ìŠ¤í¬ì¸ ', 'BTS', 'ì†í¥ë¯¼', 'ì‚¼ì„±', 'LG', 'í˜„ëŒ€', 'ì¹´ì¹´ì˜¤', 'ë„¤ì´ë²„', 'IT',
            'ì½”ë¡œë‚˜19', 'ë°±ì‹ ', 'ë¶€ë™ì‚°', 'ì•„íŒŒíŠ¸', 'ì „ì„¸', 'ëŒ€ì¶œ', 'ê¸ˆë¦¬', 'ë¹„íŠ¸ì½”ì¸', 'ì£¼ì‹íˆ¬ì',
            'ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'ì¹´ì¹´ì˜¤ë±…í¬', 'ë°°ë‹¬ìŒì‹', 'ì¹˜í‚¨', 'í”¼ì', 'ì¡±ë°œ', 'ë³´ìŒˆ',
            'ë–¡ë³¶ì´', 'ë¼ë©´', 'ë„·í”Œë¦­ìŠ¤', 'ìœ íŠœë¸Œ', 'ì¸ìŠ¤íƒ€ê·¸ë¨', 'í‹±í†¡', 'ì¹´ì¹´ì˜¤í†¡', 'ì•„ì´í°',
            'ê°¤ëŸ­ì‹œ', 'ë§¥ë¶', 'LGê·¸ë¨', 'ì—ì–´íŒŸ', 'TV'
        ]
        
    async def create_naver_service_tables(self):
        """ë„¤ì´ë²„ ì„œë¹„ìŠ¤ ìŠ¤íƒ€ì¼ í…Œì´ë¸” ìƒì„±"""
        print("ğŸ—ï¸ ë„¤ì´ë²„ ì„œë¹„ìŠ¤ í…Œì´ë¸” ìƒì„± ì¤‘...")
        
        async with aiomysql.connect(**self.db_config) as conn:
            async with conn.cursor() as cursor:
                
                await cursor.execute("""
                    CREATE TABLE IF NOT EXISTS naver_users (
                        user_id BIGINT AUTO_INCREMENT PRIMARY KEY,
                        naver_id VARCHAR(50) NOT NULL UNIQUE,
                        email VARCHAR(100) NOT NULL,
                        phone VARCHAR(20),
                        name VARCHAR(50),
                        birth_date DATE,
                        gender ENUM('M', 'F', 'N') DEFAULT 'N',
                        join_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        last_login TIMESTAMP,
                        login_count INT DEFAULT 0,
                        status ENUM('active', 'dormant', 'withdrawn') DEFAULT 'active',
                        region VARCHAR(20),
                        
                        INDEX idx_naver_id (naver_id),
                        INDEX idx_email (email),
                        INDEX idx_join_date (join_date),
                        INDEX idx_last_login (last_login),
                        INDEX idx_status (status)
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
                """)
                
                await cursor.execute("""
                    CREATE TABLE IF NOT EXISTS blog_posts (
                        post_id BIGINT AUTO_INCREMENT PRIMARY KEY,
                        user_id BIGINT NOT NULL,
                        title VARCHAR(200) NOT NULL,
                        content LONGTEXT,
                        category VARCHAR(50),
                        view_count INT DEFAULT 0,
                        like_count INT DEFAULT 0,
                        comment_count INT DEFAULT 0,
                        share_count INT DEFAULT 0,
                        is_public BOOLEAN DEFAULT true,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        
                        INDEX idx_user_id (user_id),
                        INDEX idx_created_at (created_at),
                        INDEX idx_category (category),
                        INDEX idx_view_count (view_count),
                        FOREIGN KEY (user_id) REFERENCES naver_users(user_id) ON DELETE CASCADE
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
                """)
                
                await cursor.execute("""
                    CREATE TABLE IF NOT EXISTS search_logs (
                        log_id BIGINT AUTO_INCREMENT PRIMARY KEY,
                        user_id BIGINT NULL,
                        search_query VARCHAR(500) NOT NULL,
                        search_type ENUM('web', 'image', 'news', 'shopping', 'blog', 'cafe', 'video') DEFAULT 'web',
                        results_count INT DEFAULT 0,
                        click_position INT DEFAULT 0,
                        search_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        ip_address VARCHAR(45),
                        user_agent TEXT,
                        session_id VARCHAR(100),
                        
                        INDEX idx_user_id (user_id),
                        INDEX idx_search_time (search_time),
                        INDEX idx_search_type (search_type),
                        INDEX idx_search_query (search_query(100)),
                        FOREIGN KEY (user_id) REFERENCES naver_users(user_id) ON DELETE SET NULL
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
                """)
                
                await cursor.execute("""
                    CREATE TABLE IF NOT EXISTS shopping_orders (
                        order_id BIGINT AUTO_INCREMENT PRIMARY KEY,
                        user_id BIGINT NOT NULL,
                        product_name VARCHAR(200) NOT NULL,
                        product_category VARCHAR(50),
                        product_price DECIMAL(12,2) NOT NULL,
                        quantity INT DEFAULT 1,
                        total_amount DECIMAL(12,2) NOT NULL,
                        order_status ENUM('pending', 'paid', 'shipped', 'delivered', 'cancelled') DEFAULT 'pending',
                        payment_method ENUM('card', 'bank', 'naver_pay', 'payco') DEFAULT 'card',
                        order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        shipped_date TIMESTAMP NULL,
                        delivered_date TIMESTAMP NULL,
                        seller_id VARCHAR(50),
                        
                        INDEX idx_user_id (user_id),
                        INDEX idx_order_date (order_date),
                        INDEX idx_order_status (order_status),
                        INDEX idx_total_amount (total_amount),
                        FOREIGN KEY (user_id) REFERENCES naver_users(user_id) ON DELETE CASCADE
                    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4
                """)
                
                await conn.commit()
        
        print("âœ… ë„¤ì´ë²„ ì„œë¹„ìŠ¤ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
    
    async def generate_naver_users(self, count: int = 10000) -> GenerationStats:
        """ë„¤ì´ë²„ ì‚¬ìš©ì ë°ì´í„° ìƒì„±"""
        print(f"ğŸ‘¥ {count:,}ëª…ì˜ ë„¤ì´ë²„ ì‚¬ìš©ì ìƒì„± ì¤‘...")
        
        start_time = datetime.now()
        generated_count = 0
        batch_size = 1000
        
        async with aiomysql.connect(**self.db_config) as conn:
            async with conn.cursor() as cursor:
                
                for i in range(0, count, batch_size):
                    batch_data = []
                    current_batch_size = min(batch_size, count - i)
                    
                    for j in range(current_batch_size):
                        naver_id = f"{self.fake_en.user_name()}{random.randint(1000, 9999)}"
                        email_domain = random.choice(self.naver_domains)
                        email = f"{naver_id}@{email_domain}"
                        
                        if random.random() < 0.7:
                            name = self.fake_kr.name()
                        else:
                            name = self.fake_en.name()
                        
                        birth_date = self.fake_kr.date_of_birth(minimum_age=14, maximum_age=80)
                        gender = random.choices(['M', 'F', 'N'], weights=[45, 45, 10])[0]
                        join_date = self.fake_kr.date_time_between(start_date='-5y', end_date='now')
                        
                        if random.random() < 0.95:
                            last_login = self.fake_kr.date_time_between(start_date=join_date, end_date='now')
                            login_count = random.randint(1, 1000)
                        else:
                            last_login = None
                            login_count = 0
                        
                        status = random.choices(['active', 'dormant', 'withdrawn'], weights=[80, 15, 5])[0]
                        regions = ['ì„œìš¸', 'ë¶€ì‚°', 'ì¸ì²œ', 'ëŒ€êµ¬', 'ëŒ€ì „', 'ê´‘ì£¼', 'ìš¸ì‚°', 'ê²½ê¸°', 'ê°•ì›', 'ì œì£¼']
                        region = random.choice(regions)
                        
                        batch_data.append((
                            naver_id, email, self.fake_kr.phone_number(), name, birth_date, gender, 
                            join_date, last_login, login_count, status, region
                        ))
                    
                    await cursor.executemany("""
                        INSERT INTO naver_users 
                        (naver_id, email, phone, name, birth_date, gender, 
                         join_date, last_login, login_count, status, region)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, batch_data)
                    
                    generated_count += current_batch_size
                    
                    if generated_count % 5000 == 0:
                        progress = (generated_count / count) * 100
                        print(f"   ì§„í–‰ë¥ : {generated_count:,}/{count:,} ({progress:.0f}%)")
                
                await conn.commit()
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        rate = generated_count / duration if duration > 0 else 0
        
        print(f"âœ… ì‚¬ìš©ì ë°ì´í„° ìƒì„± ì™„ë£Œ: {generated_count:,}ëª… ({rate:.1f}/ì´ˆ)")
        
        return GenerationStats(
            table_name="naver_users", target_count=count, generated_count=generated_count,
            start_time=start_time, end_time=end_time, duration_seconds=duration, generation_rate=rate
        )
    
    async def generate_blog_posts(self, count: int = 50000) -> GenerationStats:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„±"""
        print(f"ğŸ“ {count:,}ê°œì˜ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
        
        async with aiomysql.connect(**self.db_config) as conn:
            async with conn.cursor() as cursor:
                await cursor.execute("SELECT MIN(user_id), MAX(user_id) FROM naver_users WHERE status = 'active'")
                result = await cursor.fetchone()
                
                if result and result[0]:
                    min_user_id, max_user_id = result
                else:
                    raise Exception("í™œì„± ì‚¬ìš©ìê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        start_time = datetime.now()
        generated_count = 0
        batch_size = 2000
        
        async with aiomysql.connect(**self.db_config) as conn:
            async with conn.cursor() as cursor:
                
                for i in range(0, count, batch_size):
                    batch_data = []
                    current_batch_size = min(batch_size, count - i)
                    
                    for j in range(current_batch_size):
                        user_id = random.randint(min_user_id, max_user_id)
                        
                        if random.random() < 0.7:
                            title = self.fake_kr.sentence(nb_words=random.randint(3, 12))[:-1]
                        else:
                            title = self.fake_en.sentence(nb_words=random.randint(3, 10))[:-1]
                        
                        category = random.choice(self.blog_categories)
                        
                        paragraph_count = random.randint(1, 5)
                        if random.random() < 0.7:
                            content = '\n\n'.join([self.fake_kr.paragraph() for _ in range(paragraph_count)])
                        else:
                            content = '\n\n'.join([self.fake_en.paragraph() for _ in range(paragraph_count)])
                        
                        popularity = random.choices([1, 2, 3, 4, 5], weights=[50, 25, 15, 8, 2])[0]
                        
                        if popularity == 1:
                            view_count = random.randint(0, 500)
                            like_count = random.randint(0, view_count // 20)
                        elif popularity == 2:
                            view_count = random.randint(500, 2000)
                            like_count = random.randint(view_count // 20, view_count // 10)
                        elif popularity == 3:
                            view_count = random.randint(2000, 10000)
                            like_count = random.randint(view_count // 15, view_count // 8)
                        elif popularity == 4:
                            view_count = random.randint(10000, 50000)
                            like_count = random.randint(view_count // 12, view_count // 6)
                        else:
                            view_count = random.randint(50000, 500000)
                            like_count = random.randint(view_count // 10, view_count // 5)
                        
                        comment_count = random.randint(0, like_count // 2)
                        share_count = random.randint(0, comment_count * 2)
                        is_public = random.choice([True, True, True, False])
                        created_at = self.fake_kr.date_time_between(start_date='-2y', end_date='now')
                        
                        batch_data.append((
                            user_id, title, content, category, view_count, like_count,
                            comment_count, share_count, is_public, created_at
                        ))
                    
                    await cursor.executemany("""
                        INSERT INTO blog_posts 
                        (user_id, title, content, category, view_count, like_count,
                         comment_count, share_count, is_public, created_at)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, batch_data)
                    
                    generated_count += current_batch_size
                    
                    if generated_count % 10000 == 0:
                        progress = (generated_count / count) * 100
                        print(f"   ì§„í–‰ë¥ : {generated_count:,}/{count:,} ({progress:.0f}%)")
                
                await conn.commit()
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        rate = generated_count / duration if duration > 0 else 0
        
        print(f"âœ… ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {generated_count:,}ê°œ ({rate:.1f}/ì´ˆ)")
        
        return GenerationStats(
            table_name="blog_posts", target_count=count, generated_count=generated_count,
            start_time=start_time, end_time=end_time, duration_seconds=duration, generation_rate=rate
        )
    
    async def generate_search_logs(self, count: int = 200000) -> GenerationStats:
        """ë„¤ì´ë²„ ê²€ìƒ‰ ë¡œê·¸ ìƒì„±"""
        print(f"ğŸ” {count:,}ê°œì˜ ê²€ìƒ‰ ë¡œê·¸ ìƒì„± ì¤‘...")
        
        async with aiomysql.connect(**self.db_config) as conn:
            async with conn.cursor() as cursor:
                await cursor.execute("SELECT MIN(user_id), MAX(user_id) FROM naver_users")
                result = await cursor.fetchone()
                min_user_id, max_user_id = result if result else (1, 1000)
        
        start_time = datetime.now()
        generated_count = 0
        batch_size = 5000
        
        async with aiomysql.connect(**self.db_config) as conn:
            async with conn.cursor() as cursor:
                
                for i in range(0, count, batch_size):
                    batch_data = []
                    current_batch_size = min(batch_size, count - i)
                    
                    for j in range(current_batch_size):
                        user_id = random.randint(min_user_id, max_user_id) if random.random() < 0.4 else None
                        
                        if random.random() < 0.7:
                            keyword_count = random.choices([1, 2, 3], weights=[30, 50, 20])[0]
                            selected_keywords = random.sample(self.search_keywords, min(keyword_count, len(self.search_keywords)))
                            search_query = ' '.join(selected_keywords)
                        else:
                            search_query = random.choice(self.search_keywords)
                        
                        if random.random() < 0.1:
                            regions = ['ê°•ë‚¨', 'í™ëŒ€', 'ëª…ë™', 'ì´íƒœì›', 'ì‹ ì´Œ', 'ë¶€ì‚°', 'ëŒ€êµ¬']
                            search_query += f' {random.choice(regions)}'
                        
                        search_type = random.choices(
                            ['web', 'image', 'news', 'shopping', 'blog', 'cafe', 'video'],
                            weights=[60, 15, 8, 7, 4, 3, 3]
                        )[0]
                        
                        if search_type == 'web':
                            results_count = random.randint(100, 50000)
                        elif search_type == 'shopping':
                            results_count = random.randint(10, 5000)
                        else:
                            results_count = random.randint(0, 10000)
                        
                        if results_count > 0 and random.random() < 0.8:
                            click_weights = [40, 20, 15, 10, 8, 4, 2, 1]
                            max_position = min(8, results_count)
                            click_position = random.choices(
                                range(1, max_position + 1), 
                                weights=click_weights[:max_position]
                            )[0]
                        else:
                            click_position = 0
                        
                        search_time = self.fake_kr.date_time_between(start_date='-1y', end_date='now')
                        ip_address = self.fake_kr.ipv4()
                        user_agent = self.fake_en.user_agent()
                        session_id = f"sess_{random.randint(100000, 999999)}_{int(time.time())}"
                        
                        batch_data.append((
                            user_id, search_query, search_type, results_count,
                            click_position, search_time, ip_address, user_agent, session_id
                        ))
                    
                    await cursor.executemany("""
                        INSERT INTO search_logs 
                        (user_id, search_query, search_type, results_count,
                         click_position, search_time, ip_address, user_agent, session_id)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, batch_data)
                    
                    generated_count += current_batch_size
                    
                    if generated_count % 50000 == 0:
                        progress = (generated_count / count) * 100
                        elapsed = (datetime.now() - start_time).total_seconds()
                        current_rate = generated_count / elapsed if elapsed > 0 else 0
                        print(f"   ì§„í–‰ë¥ : {generated_count:,}/{count:,} ({progress:.0f}%) - {current_rate:.0f}/ì´ˆ")
                
                await conn.commit()
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        rate = generated_count / duration if duration > 0 else 0
        
        print(f"âœ… ê²€ìƒ‰ ë¡œê·¸ ìƒì„± ì™„ë£Œ: {generated_count:,}ê°œ ({rate:.1f}/ì´ˆ)")
        
        return GenerationStats(
            table_name="search_logs", target_count=count, generated_count=generated_count,
            start_time=start_time, end_time=end_time, duration_seconds=duration, generation_rate=rate
        )
    
    async def generate_shopping_orders(self, count: int = 80000) -> GenerationStats:
        """ë„¤ì´ë²„ ì‡¼í•‘ ì£¼ë¬¸ ë°ì´í„° ìƒì„±"""
        print(f"ğŸ›’ {count:,}ê°œì˜ ì‡¼í•‘ ì£¼ë¬¸ ìƒì„± ì¤‘...")
        
        product_categories = {
            'íŒ¨ì…˜ì˜ë¥˜': (10000, 200000),
            'í™”ì¥í’ˆ': (5000, 150000), 
            'ì‹í’ˆ': (3000, 50000),
            'ìƒí™œìš©í’ˆ': (2000, 100000),
            'ê°€ì „ì œí’ˆ': (50000, 500000),
            'ë„ì„œ': (5000, 50000),
            'ìŠ¤í¬ì¸ ': (20000, 300000),
            'ë””ì§€í„¸': (10000, 1000000)
        }
        
        async with aiomysql.connect(**self.db_config) as conn:
            async with conn.cursor() as cursor:
                await cursor.execute("SELECT MIN(user_id), MAX(user_id) FROM naver_users WHERE status = 'active'")
                result = await cursor.fetchone()
                min_user_id, max_user_id = result if result and result[0] else (1, 1000)
        
        start_time = datetime.now()
        generated_count = 0
        batch_size = 3000
        
        async with aiomysql.connect(**self.db_config) as conn:
            async with conn.cursor() as cursor:
                
                for i in range(0, count, batch_size):
                    batch_data = []
                    current_batch_size = min(batch_size, count - i)
                    
                    for j in range(current_batch_size):
                        user_id = random.randint(min_user_id, max_user_id)
                        
                        category = random.choice(list(product_categories.keys()))
                        price_range = product_categories[category]
                        
                        if category == 'íŒ¨ì…˜ì˜ë¥˜':
                            products = ['í›„ë“œí‹°', 'ì²­ë°”ì§€', 'ì›í”¼ìŠ¤', 'ì½”íŠ¸', 'ìŠ¤ë‹ˆì»¤ì¦ˆ']
                            brands = ['ë‚˜ì´í‚¤', 'ì•„ë””ë‹¤ìŠ¤', 'ìœ ë‹ˆí´ë¡œ', 'H&M', 'ìë¼']
                        elif category == 'í™”ì¥í’ˆ':
                            products = ['ë¦½ìŠ¤í‹±', 'íŒŒìš´ë°ì´ì…˜', 'ì•„ì´ì„€ë„', 'ë§ˆìŠ¤ì¹´ë¼']
                            brands = ['ì´ë‹ˆìŠ¤í”„ë¦¬', 'ì—ë›°ë“œ', 'MAC', 'ë‘ì½¤']
                        elif category == 'ê°€ì „ì œí’ˆ':
                            products = ['ì—ì–´í”„ë¼ì´ì–´', 'ë¯¹ì„œê¸°', 'ì²­ì†Œê¸°', 'ê³µê¸°ì²­ì •ê¸°']
                            brands = ['ì‚¼ì„±', 'LG', 'ë‹¤ì´ìŠ¨', 'í•„ë¦½ìŠ¤']
                        else:
                            products = ['ìƒí’ˆA', 'ìƒí’ˆB', 'ìƒí’ˆC']
                            brands = ['ë¸Œëœë“œA', 'ë¸Œëœë“œB', 'ë¸Œëœë“œC']
                        
                        product_name = f"{random.choice(brands)} {random.choice(products)}"
                        min_price, max_price = price_range
                        product_price = random.randint(min_price, max_price)
                        quantity = random.choices([1, 2, 3, 4, 5], weights=[60, 25, 8, 4, 3])[0]
                        total_amount = product_price * quantity
                        
                        order_status = random.choices(
                            ['delivered', 'shipped', 'paid', 'pending', 'cancelled'],
                            weights=[70, 15, 8, 4, 3]
                        )[0]
                        
                        payment_method = random.choices(
                            ['card', 'naver_pay', 'bank', 'payco'],
                            weights=[50, 25, 15, 10]
                        )[0]
                        
                        order_date = self.fake_kr.date_time_between(start_date='-1y', end_date='now')
                        
                        shipped_date = None
                        delivered_date = None
                        
                        if order_status in ['shipped', 'delivered']:
                            shipped_date = order_date + timedelta(days=random.randint(1, 3))
                            
                        if order_status == 'delivered':
                            delivered_date = shipped_date + timedelta(days=random.randint(1, 5))
                        
                        seller_id = f"seller_{random.randint(1000, 9999)}"
                        
                        batch_data.append((
                            user_id, product_name, category, product_price, quantity,
                            total_amount, order_status, payment_method, order_date,
                            shipped_date, delivered_date, seller_id
                        ))
                    
                    await cursor.executemany("""
                        INSERT INTO shopping_orders 
                        (user_id, product_name, product_category, product_price, quantity,
                         total_amount, order_status, payment_method, order_date,
                         shipped_date, delivered_date, seller_id)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, batch_data)
                    
                    generated_count += current_batch_size
                    
                    if generated_count % 20000 == 0:
                        progress = (generated_count / count) * 100
                        print(f"   ì§„í–‰ë¥ : {generated_count:,}/{count:,} ({progress:.0f}%)")
                
                await conn.commit()
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        rate = generated_count / duration if duration > 0 else 0
        
        print(f"âœ… ì‡¼í•‘ ì£¼ë¬¸ ìƒì„± ì™„ë£Œ: {generated_count:,}ê°œ ({rate:.1f}/ì´ˆ)")
        
        return GenerationStats(
            table_name="shopping_orders", target_count=count, generated_count=generated_count,
            start_time=start_time, end_time=end_time, duration_seconds=duration, generation_rate=rate
        )
    
    async def generate_all_test_data(self, scale: str = 'small') -> Dict[str, GenerationStats]:
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
        
        scales = {
            'small': {'users': 10000, 'blog_posts': 50000, 'search_logs': 200000, 'shopping_orders': 80000},
            'medium': {'users': 100000, 'blog_posts': 500000, 'search_logs': 2000000, 'shopping_orders': 800000},
            'large': {'users': 1000000, 'blog_posts': 5000000, 'search_logs': 20000000, 'shopping_orders': 8000000}
        }
        
        if scale not in scales:
            scale = 'small'
        
        target_counts = scales[scale]
        print(f"ğŸ¯ {scale.upper()} ìŠ¤ì¼€ì¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì‹œì‘")
        for key, value in target_counts.items():
            print(f"   {key}: {value:,}")
        
        total_start_time = datetime.now()
        results = {}
        
        try:
            await self.create_naver_service_tables()
            results['users'] = await self.generate_naver_users(target_counts['users'])
            results['blog_posts'] = await self.generate_blog_posts(target_counts['blog_posts'])
            results['search_logs'] = await self.generate_search_logs(target_counts['search_logs'])
            results['shopping_orders'] = await self.generate_shopping_orders(target_counts['shopping_orders'])
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return results
        
        total_end_time = datetime.now()
        total_duration = (total_end_time - total_start_time).total_seconds()
        total_records = sum(stats.generated_count for stats in results.values())
        total_rate = total_records / total_duration if total_duration > 0 else 0
        
        print(f"\nğŸ‰ ì „ì²´ ë°ì´í„° ìƒì„± ì™„ë£Œ!")
        print(f"   ì´ ë ˆì½”ë“œ: {total_records:,}ê°œ")
        print(f"   ì´ ì†Œìš” ì‹œê°„: {total_duration:.1f}ì´ˆ")
        print(f"   í‰ê·  ìƒì„±ë¥ : {total_rate:.1f}ê°œ/ì´ˆ")
        
        print(f"\nğŸ“Š í…Œì´ë¸”ë³„ ìƒì„± í†µê³„:")
        for table_name, stats in results.items():
            success_rate = (stats.generated_count / stats.target_count) * 100
            print(f"   {table_name}: {stats.generated_count:,}/{stats.target_count:,} ({success_rate:.1f}%) - {stats.generation_rate:.1f}/ì´ˆ")
        
        return results

async def quick_generate_test_data(db_config: Dict, scale: str = 'small'):
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
    generator = NaverStyleDataGenerator(db_config)
    return await generator.generate_all_test_data(scale)

async def example_usage():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    db_config = {
        'host': 'localhost',
        'port': 3306,
        'user': 'root',
        'password': 'testpass',
        'db': 'testdb'
    }
    
    print("=" * 60)
    print("ë„¤ì´ë²„ ìŠ¤íƒ€ì¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸° ì˜ˆì‹œ")
    print("=" * 60)
    
    generator = NaverStyleDataGenerator(db_config)
    
    print(f"\nğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼:")
    print(f"   small: ì‚¬ìš©ì 1ë§Œëª… + ê´€ë ¨ ë°ì´í„°")
    print(f"   medium: ì‚¬ìš©ì 10ë§Œëª… + ê´€ë ¨ ë°ì´í„°")  
    print(f"   large: ì‚¬ìš©ì 100ë§Œëª… + ê´€ë ¨ ë°ì´í„°")
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    results = await generator.generate_all_test_data(scale='small')
    
    print(f"\nâœ… ì˜ˆì‹œ ì‹¤í–‰ ì™„ë£Œ!")
    return results

if __name__ == "__main__":
    asyncio.run(example_usage())