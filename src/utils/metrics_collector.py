"""
ë„¤ì´ë²„ MySQL ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ ë° ë¶„ì„
"""
import asyncio
import time
import json
from typing import Dict, List, Optional
from datetime import datetime
from dataclasses import dataclass, asdict

import aiomysql
import psutil

@dataclass
class SystemMetrics:
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: float
    cpu_percent: float
    memory_mb: float
    disk_usage_percent: float
    network_sent_mb: float
    network_recv_mb: float

@dataclass 
class DatabaseMetrics:
    """ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: float
    server_type: str  # 'master' or 'slave'
    connections_active: int
    connections_max: int
    queries_per_second: float
    slow_queries: int
    innodb_buffer_hit_rate: float
    replication_lag_seconds: Optional[float] = None

class MetricsCollector:
    """ì¢…í•© ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, db_configs: List[Dict]):
        self.db_configs = db_configs
        self.system_metrics_history = []
        self.database_metrics_history = []
        self.is_collecting = False
        
        # ì´ì „ ë„¤íŠ¸ì›Œí¬ í†µê³„ (ë³€í™”ëŸ‰ ê³„ì‚°ìš©)
        self.prev_network_stats = psutil.net_io_counters()
        
    async def start_collection(self, interval_seconds: int = 10):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘"""
        print("ğŸ“Š ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘...")
        self.is_collecting = True
        
        while self.is_collecting:
            # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            system_metrics = self._collect_system_metrics()
            self.system_metrics_history.append(system_metrics)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            db_metrics = await self._collect_database_metrics()
            self.database_metrics_history.extend(db_metrics)
            
            # ìµœëŒ€ 1000ê°œ í•­ëª©ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
            if len(self.system_metrics_history) > 1000:
                self.system_metrics_history = self.system_metrics_history[-1000:]
            if len(self.database_metrics_history) > 1000:
                self.database_metrics_history = self.database_metrics_history[-1000:]
            
            await asyncio.sleep(interval_seconds)
    
    def stop_collection(self):
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ì§€"""
        self.is_collecting = False
        print("ğŸ“Š ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ì§€ë¨")
    
    def _collect_system_metrics(self) -> SystemMetrics:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
            memory = psutil.virtual_memory()
            memory_mb = memory.used / 1024 / 1024
            
            # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
            disk = psutil.disk_usage('/')
            disk_usage_percent = (disk.used / disk.total) * 100
            
            # ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©ëŸ‰ (ë³€í™”ëŸ‰)
            current_network = psutil.net_io_counters()
            network_sent_mb = (current_network.bytes_sent - self.prev_network_stats.bytes_sent) / 1024 / 1024
            network_recv_mb = (current_network.bytes_recv - self.prev_network_stats.bytes_recv) / 1024 / 1024
            self.prev_network_stats = current_network
            
            return SystemMetrics(
                timestamp=time.time(),
                cpu_percent=cpu_percent,
                memory_mb=memory_mb,
                disk_usage_percent=disk_usage_percent,
                network_sent_mb=max(0, network_sent_mb),  # ìŒìˆ˜ ë°©ì§€
                network_recv_mb=max(0, network_recv_mb)
            )
            
        except Exception as e:
            print(f"ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return SystemMetrics(
                timestamp=time.time(),
                cpu_percent=0, memory_mb=0, disk_usage_percent=0,
                network_sent_mb=0, network_recv_mb=0
            )
    
    async def _collect_database_metrics(self) -> List[DatabaseMetrics]:
        """ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        metrics = []
        
        for i, config in enumerate(self.db_configs):
            server_type = "master" if i == 0 else f"slave_{i}"
            
            try:
                async with aiomysql.connect(**config) as conn:
                    async with conn.cursor(aiomysql.DictCursor) as cursor:
                        db_metrics = await self._collect_single_db_metrics(cursor, server_type)
                        metrics.append(db_metrics)
                        
            except Exception as e:
                print(f"DB ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜ ({server_type}): {e}")
                # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ë©”íŠ¸ë¦­ ìƒì„±
                metrics.append(DatabaseMetrics(
                    timestamp=time.time(),
                    server_type=server_type,
                    connections_active=0,
                    connections_max=0,
                    queries_per_second=0,
                    slow_queries=0,
                    innodb_buffer_hit_rate=0
                ))
        
        return metrics
    
    async def _collect_single_db_metrics(self, cursor, server_type: str) -> DatabaseMetrics:
        """ë‹¨ì¼ DB ì„œë²„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        timestamp = time.time()
        
        # í™œì„± ì—°ê²° ìˆ˜
        await cursor.execute("SHOW STATUS LIKE 'Threads_connected'")
        connections_active = int((await cursor.fetchone())['Value'])
        
        # ìµœëŒ€ ì—°ê²° ìˆ˜
        await cursor.execute("SHOW VARIABLES LIKE 'max_connections'")
        connections_max = int((await cursor.fetchone())['Value'])
        
        # QPS ê³„ì‚°
        await cursor.execute("SHOW STATUS LIKE 'Questions'")
        questions = int((await cursor.fetchone())['Value'])
        await cursor.execute("SHOW STATUS LIKE 'Uptime'")
        uptime = int((await cursor.fetchone())['Value'])
        qps = questions / uptime if uptime > 0 else 0
        
        # Slow Queries
        await cursor.execute("SHOW STATUS LIKE 'Slow_queries'")
        slow_queries = int((await cursor.fetchone())['Value'])
        
        # InnoDB Buffer Pool Hit Rate
        await cursor.execute("SHOW STATUS LIKE 'Innodb_buffer_pool_read_requests'")
        read_requests = int((await cursor.fetchone())['Value'])
        await cursor.execute("SHOW STATUS LIKE 'Innodb_buffer_pool_reads'")
        reads = int((await cursor.fetchone())['Value'])
        
        hit_rate = (1 - reads/read_requests) * 100 if read_requests > 0 else 0
        
        # ë³µì œ ì§€ì—° (Slaveì¸ ê²½ìš°)
        replication_lag = None
        if server_type != "master":
            try:
                await cursor.execute("SHOW SLAVE STATUS")
                slave_status = await cursor.fetchone()
                if slave_status and slave_status.get('Seconds_Behind_Master') is not None:
                    replication_lag = float(slave_status['Seconds_Behind_Master'])
            except Exception:
                pass  # ë³µì œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
        
        return DatabaseMetrics(
            timestamp=timestamp,
            server_type=server_type,
            connections_active=connections_active,
            connections_max=connections_max,
            queries_per_second=qps,
            slow_queries=slow_queries,
            innodb_buffer_hit_rate=hit_rate,
            replication_lag_seconds=replication_lag
        )
    
    def get_latest_metrics(self) -> Dict:
        """ìµœì‹  ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        latest_system = self.system_metrics_history[-1] if self.system_metrics_history else None
        latest_db = self.database_metrics_history[-len(self.db_configs):] if self.database_metrics_history else []
        
        return {
            'timestamp': datetime.now().isoformat(),
            'system': asdict(latest_system) if latest_system else None,
            'databases': [asdict(db) for db in latest_db]
        }
    
    def get_metrics_summary(self, last_minutes: int = 10) -> Dict:
        """ì§€ì • ì‹œê°„ ë™ì•ˆì˜ ë©”íŠ¸ë¦­ ìš”ì•½"""
        cutoff_time = time.time() - (last_minutes * 60)
        
        # í•´ë‹¹ ì‹œê°„ ë²”ìœ„ì˜ ë©”íŠ¸ë¦­ë§Œ í•„í„°ë§
        recent_system = [m for m in self.system_metrics_history if m.timestamp >= cutoff_time]
        recent_db = [m for m in self.database_metrics_history if m.timestamp >= cutoff_time]
        
        if not recent_system or not recent_db:
            return {"error": "ì¶©ë¶„í•œ ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìš”ì•½
        system_summary = {
            'avg_cpu_percent': sum(m.cpu_percent for m in recent_system) / len(recent_system),
            'max_cpu_percent': max(m.cpu_percent for m in recent_system),
            'avg_memory_mb': sum(m.memory_mb for m in recent_system) / len(recent_system),
            'max_memory_mb': max(m.memory_mb for m in recent_system)
        }
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë©”íŠ¸ë¦­ ìš”ì•½ (ì„œë²„ë³„)
        db_summary = {}
        for server_type in set(m.server_type for m in recent_db):
            server_metrics = [m for m in recent_db if m.server_type == server_type]
            
            db_summary[server_type] = {
                'avg_qps': sum(m.queries_per_second for m in server_metrics) / len(server_metrics),
                'max_connections': max(m.connections_active for m in server_metrics),
                'avg_buffer_hit_rate': sum(m.innodb_buffer_hit_rate for m in server_metrics) / len(server_metrics),
                'total_slow_queries': max(m.slow_queries for m in server_metrics) - min(m.slow_queries for m in server_metrics)
            }
            
            # ë³µì œ ì§€ì—° (Slaveì¸ ê²½ìš°)
            replication_lags = [m.replication_lag_seconds for m in server_metrics if m.replication_lag_seconds is not None]
            if replication_lags:
                db_summary[server_type]['avg_replication_lag'] = sum(replication_lags) / len(replication_lags)
        
        return {
            'time_range_minutes': last_minutes,
            'data_points': len(recent_system),
            'system': system_summary,
            'databases': db_summary
        }
    
    def export_metrics(self, filename: str):
        """ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        export_data = {
            'export_timestamp': datetime.now().isoformat(),
            'collection_duration_minutes': len(self.system_metrics_history) * 10 / 60,  # 10ì´ˆ ê°„ê²© ê°€ì •
            'system_metrics': [asdict(m) for m in self.system_metrics_history],
            'database_metrics': [asdict(m) for m in self.database_metrics_history]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ ë©”íŠ¸ë¦­ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {filename}")
        print(f"   ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­: {len(self.system_metrics_history)}ê°œ")
        print(f"   DB ë©”íŠ¸ë¦­: {len(self.database_metrics_history)}ê°œ")